#!/usr/bin/env python3

import argparse
import json
import re
import random
import sys

import urllib.request as urllib_request

from utils import *
from termcolor import cprint


def _regex_from_to(start, end, text):
    m = re.search("(?i)" + start + "([\S\s]+?)" + end, text).group(1)
    return m


def _download_webpage(url):
    headers = {
        'Host': 'memeinsider.co',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.11; rv:55.0) Gecko/20100101 Firefox/55.0'
    }
    req = urllib_request.Request(url, headers=headers)
    webpage = urllib_request.urlopen(req).read().decode('utf-8')
    return webpage


def _download_file(url):
    # s = requests.Session()
    # r = s.get(url).content
    headers = {
        'origin': 'https://memeinsider.co',
        'referer': 'https://memeinsider.co/release/latest/isolated',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.11; rv:55.0) Gecko/20100101 Firefox/55.0',
    }
    req = urllib_request.Request(url, headers=headers)
    file = urllib_request.urlopen(req).read()
    # req = urllib_request.urlopen(url)
    return file


def _save_pdf(file_name, raw_file):
    with open(f'{file_name}', 'wb') as pdf:
        pdf.write(raw_file)
        log_success('[*] Done')


def get_latest_issue():
    url = 'https://memeinsider.co/release/latest/isolated'
    log_info('[+] Downloading webpage...')
    webpage = _download_webpage(url)
    if not webpage:
        log_error('[-] Error downloading webpage')
    issue_url = re.search(r'var url = "([^"]+)"', webpage).group(1)
    title = issue_url.split('/')[-1]
    log_info('[+] Downloading pdf...')
    issue = _download_file(issue_url)
    if not issue:
        log_error('[-] Error downloading pdf')
    _save_pdf(title, issue)


def download_specific_issue(url):
    log_info('[+] Downloading webpage...')
    webpage = _download_webpage(url + "/isolated")
    if not webpage:
        log_error('[-] Error downloading webpage')
    issue_url = re.search(r'var url = "([^"]+)"', webpage).group(1)
    title = issue_url.split('/')[-1]
    log_info('[+] Downloading pdf...')
    issue = _download_file(issue_url)
    if not issue:
        log_error('[-] Error downloading pdf')
    _save_pdf(title, issue)


def main():
    parser = argparse.ArgumentParser(description='A cli program to download MemeInsider issues')

    group = parser.add_mutually_exclusive_group()
    group.add_argument('-i', '--issue', dest='url', help="url of issue to download",
                       action="store")
    group.add_argument('-u', '--upload', help="upload to cloud storage",
                       action="store_true")
    group.add_argument('-l', '--latest', help="get the latest issue",
                       action="store_true")

    args = parser.parse_args()
    if args.latest:
        get_latest_issue()
    if args.url:
        download_specific_issue(args.url)


if __name__ == '__main__':
    print("""
                                  _            _     __                    ____
   ____ ___  ___  ____ ___  ___  (_)___  _____(_)___/ /__  _____      ____/ / /
  / __ `__ \/ _ \/ __ `__ \/ _ \/ / __ \/ ___/ / __  / _ \/ ___/_____/ __  / /
 / / / / / /  __/ / / / / /  __/ / / / (__  ) / /_/ /  __/ /  /_____/ /_/ / /
/_/ /_/ /_/\___/_/ /_/ /_/\___/_/_/ /_/____/_/\__,_/\___/_/         \__,_/_/

""")
    main()
